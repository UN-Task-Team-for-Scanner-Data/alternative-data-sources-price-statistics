{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Web scraping example"
      ],
      "metadata": {
        "id": "_8gMDai25ozN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook provides an example about the use of web scraping to retrieve price data.\n",
        "\n",
        "Specifically, it uses the test website [Books to scrape](https://books.toscrape.com/) to demonstrate how to gather data from a website, including navigation throught the various pages. We used the browser inspection functions to analyze the HTML structure of the webpages and define the paths for data extractions. The techniques and style used here are only an example, you could achieve the same result in many different ways. Also the structure is specific for this website, you will need to adapt this code for others.\n",
        "\n",
        "There are additional resources you may explore in order to get some practice with different website architectures before applying your efforts to real websites:\n",
        "\n",
        "- [Web Scraper test sites](https://www.webscraper.io/test-sites)\n",
        "- [web-scraping.dev](https://www.web-scraping.dev/)\n",
        "\n",
        "You can run this notebook on your own environment, provided you install the Python packages listed in the requirements.txt file. You can also run this notebook on a cloud environment like Google Colab. The minimum version of Python tested with this notebook is 3.9, we suggest at least 3.10 or newer.\n",
        "\n",
        "Please be considerate in your web scraping operations and always include a delay between calls to avoid overloading the source website."
      ],
      "metadata": {
        "id": "TnHa6n5h54Wr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "pi5wNY8D8g1o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "LlVSN4jP4W23"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import random\n",
        "import urllib.parse\n",
        "from datetime import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup variables"
      ],
      "metadata": {
        "id": "6oain_aZAw6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Request headers are the primary way a website operator may identify your requests. It is good practice to use a User-Agent name which can identify your institution and leave an email for being contacted."
      ],
      "metadata": {
        "id": "T98MGMOA9A_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "heads = {\n",
        "    'User-Agent':'Scraping Project Name', # Change with the name of your project or institution\n",
        "    'email': 'your.email@institution.gov' # Change with your own email address\n",
        "    }\n",
        "\n",
        "s = requests.Session()"
      ],
      "metadata": {
        "id": "60iuflud8lI9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shop_url = \"https://books.toscrape.com/\""
      ],
      "metadata": {
        "id": "ZcQ6HpUV9jHr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Acquire category links"
      ],
      "metadata": {
        "id": "L6wLKpxMA0Xq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the homepage"
      ],
      "metadata": {
        "id": "VU5W8GUxA-N4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with s.get(shop_url, headers=heads) as res:\n",
        "    response = BeautifulSoup(res.text, \"html.parser\")"
      ],
      "metadata": {
        "id": "9A10I7iT9lye"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the list of categories, excluding the full catalogue (Which is in a different `ul` element)"
      ],
      "metadata": {
        "id": "Mux3zfKL_A0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categories = [  # We use list comprehension to iterate over the various elements\n",
        "                # You could also use a regular for-loop for the same puropose\n",
        "    {\n",
        "        \"category\": item.get_text().strip(), # We use \"strip\" to remove white space around the text\n",
        "        \"url\": urllib.parse.urljoin(shop_url, item.get(\"href\"))  # Links in the menu are relative, we need to add the website root\n",
        "    }\n",
        "    for item in\n",
        "    response.find(\"ul\", class_=\"nav-list\").find(\"ul\").find_all(\"a\")\n",
        "]"
      ],
      "metadata": {
        "id": "PDxEvstn9qxh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example category"
      ],
      "metadata": {
        "id": "3zGsIqHcAovj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categories[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSWiB_LvAbEL",
        "outputId": "b387c72d-77ca-45fd-e4f6-eac2be9ae670"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'category': 'Travel',\n",
              " 'url': 'https://books.toscrape.com/catalogue/category/books/travel_2/index.html'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Acquire product data across categories"
      ],
      "metadata": {
        "id": "yrim74sEBBaz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploration"
      ],
      "metadata": {
        "id": "tIX8KmNPBieS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before structuring the data acquisition with functions and loops, it is common practice to manually explore a few web pages to check the data structure and organization."
      ],
      "metadata": {
        "id": "UOUy4xYNGm-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with s.get(categories[1].get(\"url\"), headers=heads) as res:\n",
        "    response = BeautifulSoup(res.text, \"html.parser\")"
      ],
      "metadata": {
        "id": "t8mUz6F3Adbb"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get product data"
      ],
      "metadata": {
        "id": "I1RmUuzcFa7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "products = [\n",
        "    {\n",
        "        \"name\": item.find(\"h3\").find(\"a\").get(\"title\"), # text inside the tag is truncated in some instance\n",
        "        \"price\": item.find(\"p\", {\"class\": \"price_color\"}).get_text(),\n",
        "        \"link\": urllib.parse.urljoin(  # Also in this case links are relative\n",
        "            categories[1].get(\"url\"),\n",
        "            item.find(\"h3\").find(\"a\").get(\"href\")),\n",
        "        \"category\": categories[1].get(\"category\")\n",
        "\n",
        "    }\n",
        "    for item in\n",
        "    response.find_all(\"article\", {\"class\": \"product_pod\"})\n",
        "]"
      ],
      "metadata": {
        "id": "dGsPYNRPBqHP"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "products[:4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCxpT6g0C6B1",
        "outputId": "14a4e695-b113-4738-ab3f-f109b3ae54ab"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'Sharp Objects',\n",
              "  'price': 'Â£47.82',\n",
              "  'link': 'https://books.toscrape.com/catalogue/sharp-objects_997/index.html',\n",
              "  'category': 'Mystery'},\n",
              " {'name': 'In a Dark, Dark Wood',\n",
              "  'price': 'Â£19.63',\n",
              "  'link': 'https://books.toscrape.com/catalogue/in-a-dark-dark-wood_963/index.html',\n",
              "  'category': 'Mystery'},\n",
              " {'name': 'The Past Never Ends',\n",
              "  'price': 'Â£56.50',\n",
              "  'link': 'https://books.toscrape.com/catalogue/the-past-never-ends_942/index.html',\n",
              "  'category': 'Mystery'},\n",
              " {'name': 'A Murder in Time',\n",
              "  'price': 'Â£16.64',\n",
              "  'link': 'https://books.toscrape.com/catalogue/a-murder-in-time_877/index.html',\n",
              "  'category': 'Mystery'}]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find pagination link for next page"
      ],
      "metadata": {
        "id": "tkoERNH3FdH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response.find(\"ul\", {\"class\": \"pager\"}).find(\"li\", {\"class\": \"next\"}).find(\"a\").get(\"href\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RkzaKsgYFL8a",
        "outputId": "0cba3a27-a797-4ddf-d5bb-6860e5a0685c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'page-2.html'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next_page = urllib.parse.urljoin( # Join relative link\n",
        "    categories[1].get(\"url\"),\n",
        "    response.find(\"ul\", {\"class\": \"pager\"}).find(\"li\", {\"class\": \"next\"}).find(\"a\").get(\"href\"))"
      ],
      "metadata": {
        "id": "H11mEINzFmod"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next_page"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EDOWbcEKFvv-",
        "outputId": "0dcdf177-850a-49af-b54a-39c18f153008"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Systematic data acquisition"
      ],
      "metadata": {
        "id": "SUJslZq-GB-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the exploration, it is common practice to arrange the data acquisition using functions and loops. The function below acquire data from a category page and recursively extend the acquisition to following pages if they exist."
      ],
      "metadata": {
        "id": "lbpdN7N-H0rA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_product_data(category: dict, s: requests.Session, heads: dict) -> list:\n",
        "    \"\"\"\n",
        "    Get product data from a category page and recursively extend\n",
        "    the acquisition to following pages if they exist.\n",
        "    Arguments:\n",
        "    category (dict): dictionary with category name and url\n",
        "    s (requests.Session): session object\n",
        "    heads (dict): request headers\n",
        "\n",
        "    Returns:\n",
        "    results: list of product data\n",
        "    \"\"\"\n",
        "    # Random delay to prevent server overloading\n",
        "    time.sleep(random.randint(10, 15)/10)\n",
        "    # Get category webpage\n",
        "    with s.get(category.get(\"url\"), headers=heads) as res:\n",
        "        response = BeautifulSoup(res.text, \"html.parser\")\n",
        "    # Extract product data\n",
        "    results = [\n",
        "        {\n",
        "            \"name\": item.find(\"h3\").find(\"a\").get(\"title\"), # text inside the tag is truncated in some instance\n",
        "            \"price\": item.find(\"p\", {\"class\": \"price_color\"}).get_text(),\n",
        "            \"link\": urllib.parse.urljoin(  # Also in this case links are relative\n",
        "                category.get(\"url\"),\n",
        "                item.find(\"h3\").find(\"a\").get(\"href\")),\n",
        "            \"category\": category.get(\"category\"),\n",
        "            \"date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
        "\n",
        "        }\n",
        "        for item in\n",
        "        response.find_all(\"article\", {\"class\": \"product_pod\"})\n",
        "    ]\n",
        "    # Check if there is a next page, navigating the structure step by step\n",
        "    # First check if there is a pagination area\n",
        "    next_page = response.find(\"ul\", {\"class\": \"pager\"})\n",
        "    if next_page is not None:\n",
        "        next_page = next_page.find(\"li\", {\"class\": \"next\"})\n",
        "        # Second, check if there is a next page in the navigation\n",
        "        if next_page is not None:\n",
        "            # Compose next page URL\n",
        "            new_url = urllib.parse.urljoin( # Join relative link\n",
        "                category.get(\"url\"),\n",
        "                next_page.find(\"a\").get(\"href\"))\n",
        "            # Acquire data from next page\n",
        "            new_results = get_product_data(\n",
        "                # Update category object with next page url\n",
        "                category={\"name\": category.get(\"name\"), \"url\": new_url},\n",
        "                s=s,\n",
        "                heads=heads)\n",
        "            # Join results\n",
        "            results.extend(new_results)\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "zAu3-0awFw0b"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a loop to acquire data from all categories"
      ],
      "metadata": {
        "id": "tn04npeKK4Hw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "products = []\n",
        "for category in categories:\n",
        "    # Random delay to prevent server overloading\n",
        "    time.sleep(random.randint(10, 15)/10)\n",
        "    new_products = get_product_data(category, s, heads)\n",
        "    products.extend(new_products)"
      ],
      "metadata": {
        "id": "gaZXP3tpKu10"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Arrange the results in a DataFrame and save"
      ],
      "metadata": {
        "id": "vY7x4XSoLa45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = pd.DataFrame(products)\n",
        "# Save file with current date in the name\n",
        "data_df.to_csv(\"book_prices_{}.csv\".format(datetime.now().strftime(\"%Y-%m-%d\")), index=False)"
      ],
      "metadata": {
        "id": "ZbE9hWQVLd-k"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o3LQNjHqPDtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next steps"
      ],
      "metadata": {
        "id": "RGD_iTUbL3aQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may have seen that inside each book webpage there is additional information, such as the ISBN code (which uniquely identifies each book), the number of units in stock, and more. If this information is important for you, you may want to extend the data acquisition to each product page and enrich your data.\n",
        "\n",
        "However, you should also consider the additional load you would impose on the source website with those incremental calls. Since product attributes rarely change, a more balanced aproach may be to acquire them with a lower frequency (for instance, once a month) rather than daily."
      ],
      "metadata": {
        "id": "Dl8GL1tSL6xv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Un_MpMSL6F0"
      },
      "execution_count": 33,
      "outputs": []
    }
  ]
}